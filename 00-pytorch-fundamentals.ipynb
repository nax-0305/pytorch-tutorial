{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 认识tensor\n",
    "tensor是一个三维向量，比如表示图片的时候，可以第一维度表示color channel，第二维表示length，第三维表示width。\n",
    "\n",
    "1. scalar 是一个标量\n",
    "2. vector 是一个向量，行向量或者列向量\n",
    "3. MATRIX 是一个矩阵，n*m\n",
    "4. TENSOR 是一个三维向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, torch.Size([]), 7, tensor(7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个scalar标量，也就是常数\n",
    "\n",
    "scalar = torch.tensor(7)\n",
    "scalar.ndim, scalar.shape, scalar.item(), scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2]), 7, tensor([7, 7]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个vector向量\n",
    "\n",
    "vector = torch.tensor([7, 7])\n",
    "vector.ndim, vector.shape, vector[1].item(), vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " torch.Size([2, 2]),\n",
       " 7,\n",
       " tensor([[7, 7],\n",
       "         [7, 7]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个MATRIX矩阵\n",
    "\n",
    "MATRIX = torch.tensor([[7, 7], [7, 7]])\n",
    "MATRIX.ndim, MATRIX.shape, MATRIX[0][0].item(), MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " torch.Size([3, 2, 2]),\n",
       " 1,\n",
       " tensor([[[1, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[2, 3],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[3, 4],\n",
       "          [3, 4]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个TENSOR\n",
    "\n",
    "TENSOR = torch.tensor([[[1, 2], [1, 2]], [[2, 3], [2, 3]], [[3, 4], [3, 4]]])\n",
    "TENSOR.ndim, TENSOR.shape, TENSOR[0][0][0].item(), TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他创建tensor的方法\n",
    "\n",
    "1. 随机数，tensor.rand(size=(x,x,x))\n",
    "2. 全0或全1，tensor.zeros(size=(x,x,x)), tensor.ones(size=(x,x,x))\n",
    "3. 范围值，tensor.arange(start, end, step)\n",
    "4. shape_like全0或全1，tensor.zeros_like(input=atensor), tensor.ones_like(input=atensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " torch.Size([3, 4]),\n",
       " tensor([[0.9623, 0.7569, 0.1766, 0.6042],\n",
       "         [0.2051, 0.8328, 0.8719, 0.7591],\n",
       "         [0.7760, 0.1034, 0.3656, 0.2343]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建随机数\n",
    "\n",
    "random_tensor = torch.rand(size = (3, 4))\n",
    "random_tensor.ndim, random_tensor.shape, random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建全0或者全1\n",
    "\n",
    "zeros_tensor = torch.zeros(size=(1, ))\n",
    "ones_tensor = torch.ones(size=(2, 2))\n",
    "zeros_tensor, ones_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建范围值，包含start，不包含end\n",
    "\n",
    "range_tensor = torch.arange(0, 10, 1)\n",
    "range_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]),\n",
       " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " torch.Size([10]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " torch.Size([10]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建shape_like的tensor\n",
    "\n",
    "\n",
    "zeros_like_tensor = torch.zeros_like(input = range_tensor)\n",
    "ones_like_tensor = torch.ones_like(input = range_tensor)\n",
    "range_tensor.shape, range_tensor, zeros_like_tensor.shape, zeros_like_tensor, ones_like_tensor.shape, ones_like_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor的数据类型\n",
    "\n",
    "tensor的数据类型：https://pytorch.org/docs/stable/tensors.html#data-types\n",
    "cpu和gpu不大相同\n",
    "\n",
    "通用的类型有：torch.float32/torch.float, torch.float16/torch.half, torch.float64/torch.double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]),\n",
       " torch.float32,\n",
       " device(type='cpu'),\n",
       " torch.Size([3]),\n",
       " torch.float16,\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor，可以得到的重要信息是shape，datatype(default是torch.float32)，device(default是cpu)\n",
    "\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=None, device=None, requires_grad=False)\n",
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float16)\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device, float_16_tensor.shape, float_16_tensor.dtype, float_16_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 操作tensor\n",
    "\n",
    "1. 加、减、乘、除（均是element-wise）\n",
    "2. 矩阵相乘\n",
    "3. 最小、最大、平均、求和（聚合函数）\n",
    "4. 最小、最大的下标\n",
    "5. \"redtype\"、\"reshape\"\n",
    "6. getor、setor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: tensor([1, 2, 3, 4])\n",
      "tensor + 10: (tensor([11, 12, 13, 14]), tensor([11, 12, 13, 14]))\n",
      "tensor - 10: (tensor([-9, -8, -7, -6]), tensor([-9, -8, -7, -6]))\n",
      "tensor * 10: (tensor([10, 20, 30, 40]), tensor([10, 20, 30, 40]))\n",
      "tensor / 10: (tensor([0.1000, 0.2000, 0.3000, 0.4000]), tensor([0.1000, 0.2000, 0.3000, 0.4000]))\n",
      "tensor // 10: (tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 1.1 加、减、乘、除一个常数\n",
    "# torch.add(tensor, 10), torch.sub(tensor, 10), torch.mul(tensor, 10), torch.div(tensor, 10), torch.floor_divide(tensor, 10)\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"tensor: {tensor}\")\n",
    "print(f\"tensor + 10: {(tensor + 10, torch.add(tensor, 10))}\")\n",
    "print(f\"tensor - 10: {(tensor - 10, torch.sub(tensor, 10))}\")\n",
    "print(f\"tensor * 10: {(tensor * 10, torch.mul(tensor, 10))}\")\n",
    "print(f\"tensor / 10: {(tensor / 10, torch.div(tensor, 10))}\")\n",
    "print(f\"tensor // 10: {(tensor // 10, torch.floor_divide(tensor, 10))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1 + tensor2: tensor([11, 22, 33, 44])\n",
      "tensor1 - tensor2: tensor([ -9, -18, -27, -36])\n",
      "tensor1 * tensor2: tensor([ 10,  40,  90, 160])\n",
      "tensor1 / tensor2: tensor([0.1000, 0.1000, 0.1000, 0.1000])\n",
      "tensor1 // tensor2: tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 1.2 加、减、乘、除相同shape的tensor\n",
    "# 同理，也可以使用torch.fun()\n",
    "\n",
    "tensor1 = torch.tensor([1,2,3,4])\n",
    "tensor2 = torch.tensor([10,20,30,40])\n",
    "print(f\"tensor1 + tensor2: {tensor1 + tensor2}\")\n",
    "print(f\"tensor1 - tensor2: {tensor1 - tensor2}\")\n",
    "print(f\"tensor1 * tensor2: {tensor1 * tensor2}\")\n",
    "print(f\"tensor1 / tensor2: {tensor1 / tensor2}\")\n",
    "print(f\"tensor1 // tensor2: {tensor1 // tensor2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1 @ vector2: 56\n",
      "vector3 @ vector4: 11\n",
      "vector3.T @ vector4: 11\n",
      "vector3 @ vector4.T: 11\n",
      "matrix1: tensor([[0.4036, 0.3328],\n",
      "        [0.3893, 0.6209]])\n",
      "matrix2: tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "matrix1 @ matrix2: tensor([[0.7363, 0.7363],\n",
      "        [1.0103, 1.0103]])\n"
     ]
    }
   ],
   "source": [
    "# 2 矩阵相乘\n",
    "\n",
    "# 标量无法使用矩阵乘法\n",
    "# scalar1 = torch.tensor(7)\n",
    "# scalar2 = torch.tensor(8)\n",
    "# print(f\"scalar1 @ scalar2: {scalar1 @ scalar2}\")\n",
    "\n",
    "vector1 = torch.tensor([7])\n",
    "vector2 = torch.tensor([8])\n",
    "print(f\"vector1 @ vector2: {vector1 @ vector2}\")\n",
    "\n",
    "vector3 = torch.tensor([2, 1])\n",
    "vector4 = torch.tensor([4, 3])\n",
    "print(f\"vector3 @ vector4: {vector3 @ vector4}\")\n",
    "print(f\"vector3.T @ vector4: {vector3.T @ vector4}\")\n",
    "print(f\"vector3 @ vector4.T: {vector3 @ vector4.T}\")\n",
    "\n",
    "matrix1 = torch.rand(size=(2,2))\n",
    "matrix2 = torch.ones_like(input=matrix1)\n",
    "print(f\"matrix1: {matrix1}\")\n",
    "print(f\"matrix2: {matrix2}\")\n",
    "print(f\"matrix1 @ matrix2: {matrix1 @ matrix2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: (tensor(0), tensor(0))\n",
      "Maximum: (tensor(90), tensor(90))\n",
      "Mean: (tensor(45.), tensor(45.))\n",
      "Sum: (tensor(450), tensor(450))\n"
     ]
    }
   ],
   "source": [
    "# 3 聚合函数\n",
    "\n",
    "x = torch.arange(0, 100, 10)\n",
    "print(f\"Minimum: {(x.min(), torch.min(x))}\")\n",
    "print(f\"Maximum: {(x.max(), torch.max(x))}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {(x.type(torch.float32).mean(), torch.mean(x.type(torch.float32)))}\") # won't work without float datatype\n",
    "print(f\"Sum: {(x.sum(), torch.sum(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1, 2, 3, 3])\n",
      "Index where max value occurs: (2, 2)\n",
      "Index where min value occurs: (tensor(0), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "# 4 最值下标\n",
    "\n",
    "tensor = torch.tensor([1,2,3,3])\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "# Returns index of max and min values\n",
    "print(f\"Index where max value occurs: {(tensor.argmax().item(), torch.argmax(tensor).item())}\")\n",
    "print(f\"Index where min value occurs: {(tensor.argmin(), torch.argmin(tensor))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor type is (tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), torch.int64)\n",
      "tensor type is (tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 5.1 \"redtype\"\n",
    "\n",
    "tensor = torch.arange(0, 10, 1)\n",
    "print(f\"tensor type is {(tensor, tensor.dtype)}\")\n",
    "tensor = tensor.type(torch.float32)\n",
    "print(f\"tensor type is {(tensor, tensor.dtype)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (tensor([0., 1., 2., 3., 4., 5., 6., 7.]), torch.Size([8]))\n",
      "x_shape: (tensor([[0., 1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 8]))\n",
      "x_view: (tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]]), torch.Size([2, 4]))\n",
      "y_t_shape: (tensor([[0.5107, 0.5132, 0.0295, 0.7072, 0.6833, 0.0940, 0.5767, 0.7065, 0.8965,\n",
      "         0.6741, 0.2527, 0.5032]]), torch.Size([1, 12]))\n",
      "x_stack_0 :(tensor([[100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "        [100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "        [100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "        [100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.]]), torch.Size([4, 8]))\n",
      "x_stack_1 :(tensor([[100., 100., 100., 100.],\n",
      "        [  1.,   1.,   1.,   1.],\n",
      "        [  2.,   2.,   2.,   2.],\n",
      "        [  3.,   3.,   3.,   3.],\n",
      "        [  4.,   4.,   4.,   4.],\n",
      "        [  5.,   5.,   5.,   5.],\n",
      "        [  6.,   6.,   6.,   6.],\n",
      "        [  7.,   7.,   7.,   7.]]), torch.Size([8, 4]))\n",
      "x_squeeze: tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
      "x_unsqueeze: tensor([[[0., 1., 2., 3., 4., 5., 6., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "# 5.2 reshape\n",
    "\n",
    "## 使用reshape, 使用view\n",
    "## view只能使用contiguous array\n",
    "## reshape还可以使用non-contiguous\n",
    "\n",
    "x = torch.arange(0, 8, 1).type(torch.float32)\n",
    "print(f\"x: {x, x.shape}\")\n",
    "x_shape = x.reshape((1, 8))\n",
    "x_view = x.view(2, 4)\n",
    "print(f\"x_shape: {x_shape, x_shape.shape}\")\n",
    "print(f\"x_view: {x_view, x_view.shape}\")\n",
    "\n",
    "y = torch.rand(3, 4)\n",
    "y_t_shape = y.T.reshape(-1, 12)\n",
    "print(f\"y_t_shape: {y_t_shape, y_t_shape.shape}\")\n",
    "# y.T是non-contiguous\n",
    "# y_t_view = y.T.view(2, 6)\n",
    "# RuntimeError: view size is not compatible with \n",
    "# input tensor's size and stride (at least one dimension spans across two contiguous subspaces). \n",
    "# Use .reshape(...) instead.\n",
    "# print(f\"y_t_view: {y_t_view, y_t_view.shape}\")\n",
    "\n",
    "## 使用stack增加相同的向量，分为dim = 0, dime = 1\n",
    "x_stack_0 = torch.stack([x, x, x, x], dim = 0)\n",
    "x_stack_1 = torch.stack([x, x, x, x], dim = 1)\n",
    "print(f\"x_stack_0 :{x_stacked_0, x_stacked_0.shape}\")\n",
    "print(f\"x_stack_1 :{x_stacked_1, x_stacked_1.shape}\")\n",
    "\n",
    "## 使用squeeze和unsqueeze，挤压（减少）或者不挤压（增加）外层向量\n",
    "x_squeeze = x_shape.squeeze()\n",
    "print(f\"x_squeeze: {x_squeeze}\")\n",
    "x_unsqueeze = x_shape.unsqueeze(0)\n",
    "print(f\"x_unsqueeze: {x_unsqueeze}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
